### Implementation of Fourier Neural Operators for the 1D FitzHughâ€“Nagumo System

This repository contains the code developed for my **Bachelor thesis project**, entitled  
*â€œApproximating Cardiac Action Potentials from the FitzHughâ€“Nagumo Model Using a Fourier Neural Operator : A Theoretical and Practical Studyâ€*.  
The work combines theoretical insights on operator learning with a practical implementation of a **Fourier Neural Operator (FNO)** in Python, applied to a simplified model of cardiac action potentials.

---

#### ğŸ§¬ Overview

The goal of this project is to train an FNO to approximate solutions of the **1D FitzHughâ€“Nagumo system**, a reduced model of excitable media such as cardiac tissue.  
The repository includes code for data generation, preprocessing, model definition, training, and evaluation, as well as scripts for hyperparameter exploration and visualization.

---

#### ğŸ“Š Data generation in `Data/FitzHugh_Nagumo/`

- Synthetic datasets are generated by numerically integrating the FitzHughâ€“Nagumo system with RADAU method.  
- The resulting trajectories are normalized (minmax) and stored in `./data_tensors/`.   
- For file size reasons, only the reduced version (`*_red.pt`) is stored here (500 points for each input and output curves).
- To fully regenerate the dataset, please run the following scripts in `./Data_generation/` in the following order:
  1. `dataset_generation.py` â€“ generates the full dataset and stores it as a dictionary of simulation results.
  2. `conversion_into_data_tensors.py` â€“ converts the dictionary into PyTorch tensor files for training and validation.
  3. `normalization_minmax.py` â€“ applies minâ€“max normalization across all samples.
  4. `reduction_number_points.py` â€“ reduces the temporal resolution to produce lighter dataset versions (*_red.pt).

These steps ensure a clean and reproducible data pipeline from raw numerical simulations to model-ready tensors.

---

#### ğŸ§  FNO Implementation in `FNO/FitzHugh_Nagumo/`

- The core model is based on the **`neuralop`** libraryâ€™s implementation of the **Fourier Neural Operator**.  
- Training routines, loss functions, and evaluation metrics are defined in `./FNO.py`. 
- `./Test_FNO.py` evaluates the trained model on the test set. It also includes code for visualizing a few representative test samples to assess the modelâ€™s qualitative performance.
- The file `./FNO_Hyperparameters.py` performs a two-step hyperparameter search using **Ray Tune** (quick grid search followed by fine-tuning on the top configurations).

---

#### ğŸ† Best Paths in `best_paths/`

- At the end of training, for each manual variation of hyperparameters, the best-performing configuration (i.e., the one achieving the lowest validation loss during training) is saved in the `best_paths` folder.
Each model is stored following the naming convention:
`best_fno_model_minmax_modeA_hcB_layersC_bzD_seedE.pth`
where
  - A = number of modes (`n_modes`),
  - B = number of hidden channels (`hidden_channels`),
  - C = number of layers (`n_layers`),
  - D = batch size (`batch_size`), and
  - E = random seed (`seed`).
- These different models can be tested and visualized in `Test_FNO.py` by adjusting the hyperparameters in the code (lines 14, 35, 52, 53, and 57) and specifying the corresponding model path at line 61.

---

#### âš™ï¸ Additional Tests: Dahlquist Equation

Most of the code (except for the hyperparameter search) has also been adapted to the **Dahlquist equation**.  
This simplified linear test case was used to perform controlled experiments and verify the FNOâ€™s numerical behavior under idealized conditions.

---

#### ğŸ§¾ Requirements

To run the code, install the main dependencies:

```bash
pip install torch neuraloperator ray matplotlib numpy
```
GPU acceleration is recommended for training.

---
#### ğŸ“ Repository Structure

```text
.
â”œâ”€â”€ Data/                         # Data generation and preprocessing scripts
â”‚   â”œâ”€â”€Dahlquist/                 # Data generation for Dahlquist equation
â”‚   â”œâ”€â”€FitzHugh_Nagumo/           # Data generation for FitzHugh-Nagumo equation
â”‚   â”‚   â”œâ”€â”€Data_generation/       # Data generation routine 
â”‚   â”‚   â””â”€â”€Data_tensors/          # Preprocessed reduced tensors       
â”œâ”€â”€ FNO/                          # Model definitions and training routines
â”‚   â”œâ”€â”€Dahlquist/                 # Model for Dahlquist equation
â”‚   â”œâ”€â”€FitzHugh_Nagumo/           # Model for FitzHugh-Nagumo equation
â”‚   â”‚   â”œâ”€â”€Plots/                 # Visualizations 
â”‚   â”‚   â”œâ”€â”€FNO.py                 # Training model
â”‚   â”‚   â”œâ”€â”€FNO_Hyperparameters.py # Best hyperparameters search
â”‚   â”‚   â”œâ”€â”€parameters_count.py    # Count of the number of parameters in each model
â”‚   â”‚   â””â”€â”€Test_FNO.py            # Evaluation of the trained model on the test set
â”œâ”€â”€ best_paths                    # Saved best configurations
â””â”€â”€ README.md                     # Project overview
```

#### ğŸ“š Reference

This work was carried out as part of my **Bachelor thesis** at **UniDistance Suisse**,  
under the supervision of **Dr. Marco Favino** and **Prof. Rolf Krause** *(October 2025)*.

